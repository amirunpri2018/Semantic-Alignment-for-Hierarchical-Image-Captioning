## V2.1
## Abstract
<span style="font-size: 1.5em;">Inspired by recent progress of hierarchical reinforcement learning and adversarial text generation, we introduce a hierarchical adversarial attention based model to generate natural language description of images. The model automatically learns to align the attention over images and subgoal vectors in the process of caption generation. We describe how we can train, use and understand the model by showing its performance on Flickr8k. We also visualize the subgoal vectors and attention over images during generation procedures.</span>


<img src="./assets/architecture-page-001.jpg" align="center" style="width:100%">

## Authors

<table style="width:100% bgcolor:#FFFFFF" align="center">
  <tr>
    <th><img src="./assets/lsd.JPG" style="border-radius:50%;"/></th>
    <th><img src="./assets/fzy.JPG" style="border-radius:50%;"/></th> 
    <th><img src="./assets/spy.JPG" style="border-radius:50%;"/></th>
  </tr>
 Â <tr align="center">
    <th>Sidi Lu</th>
    <th>Zhiyong Fang</th>
    <th>Peiyao Sheng</th>
  </tr>
</table>

## Demo

<p style="text-align:center"><a href="https://www.youtube.com/watch?v=QJbCfhRkcyg"><img src="https://img.youtube.com/vi/QJbCfhRkcyg/0.jpg" align="center" alt="IMAGE ALT TEXT HERE" width="75%" /></a></p>

## Code
<span border="0" style="font-size: 1.5em;" >
We provide source code on [Github](https://github.com/zhiyong1997/github-pages-test), including:
</span>
<table>
  <tr>
    <td><span style="font-size: 1.5em;"> 1. Train/Test code.</span></td>
  </tr>
  <tr>
    <td><span style="font-size: 1.5em;"> 2. Visualization tool for attention mechanism.</span></td>
  </tr>
</table>

## Paper
<span style="font-size: 1.5em;"> Our paper is available [here](https://github.com/zhiyong1997/github-pages-test/blob/master/assets/HACap.pdf)</span>

## Bibtex
<pre style="font-size: 1.5em;">
@Article{Lu2018SemanticAlignment,
          title={Semantic Alignment for Hierarchical Image Captioning},
          author={Lu, Sidi and Fang, Zhiyong and Sheng Peiyao},
          year={2018},
          archivePrefix = "arXiv",
          eprint = "0707.3168",
          primaryClass  = "hep-th"
        }
</pre>
